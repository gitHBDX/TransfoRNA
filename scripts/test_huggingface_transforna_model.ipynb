{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transforna import GeneEmbeddModel,load\n",
    "import yaml\n",
    "import torch\n",
    "mapping_dict_path: str = '/media/ftp_share/hbdx/data_for_upload/TransfoRNA/data/subclass_to_annotation.json'\n",
    "model = \"Seq-Seq\"\n",
    "model_name = f\"Yak-hbdx/{model}-TransfoRNA\"\n",
    "model_dir = f\"/nfs/home/yat_ldap/VS_Projects/TransfoRNA-Framework/models/tcga/TransfoRNA_FULL/sub_class/{model}/\"\n",
    "model_path = model_dir+\"/ckpt/model_params_tcga.pt\"\n",
    "model_config_path = model_dir+\"meta/hp_settings.yaml\"\n",
    "cfg = load(model_config_path)\n",
    "mapping_dict = load(mapping_dict_path)\n",
    "with open(model_dir+\"/seq_tokens_ids_dict.yaml\") as file:\n",
    "    token_to_ids = yaml.load(file, Loader=yaml.FullLoader)\n",
    "if 'struct' in model.lower():\n",
    "    with open(model_dir+\"/second_input_tokens_ids_dict.yaml\") as file:\n",
    "        second_input_token_to_ids = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "    token_to_ids.update(second_input_token_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#main_config = cfg#{\"train_config\":cfg['train_config'],\"model_config\":cfg['model_config']}\n",
    "cfg[\"train_config\"][\"device\"] = 'cpu'\n",
    "cfg[\"mapping_dict\"] = mapping_dict\n",
    "model = GeneEmbeddModel(cfg)\n",
    "#load state dict\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.push_to_hub(model_name)\n",
    "model = GeneEmbeddModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "import os\n",
    "import numpy as np\n",
    "class Tokenizer(PreTrainedTokenizer):\n",
    "\n",
    "    model_input_names = [\"input_ids\"]#, \"attention_mask\"]\n",
    "    do_upper_case: bool = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        do_upper_case: bool = True,\n",
    "        model_max_length: int = 30,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self._token_to_id = token_to_ids\n",
    "        self._id_to_token = {id: token for token, id in self._token_to_id.items()}\n",
    "\n",
    "        super().__init__(\n",
    "            model_max_length=model_max_length,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.do_upper_case = do_upper_case\n",
    "\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._id_to_token.get(index, None)\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._token_to_id.get(token, self._token_to_id.get(None))  # type: ignore[arg-type]\n",
    "\n",
    "    def _tokenize(self, rnas: str, **kwargs):\n",
    "        if self.do_upper_case:\n",
    "            rnas = rnas.upper()\n",
    "        return list(rnas)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self._token_to_id.copy()\n",
    "\n",
    "    def token_to_id(self, token: str) -> int:\n",
    "        return self._token_to_id.get(token, self._token_to_id.get(None))  # type: ignore[arg-type]\n",
    "\n",
    "    def id_to_token(self, index: int) -> str:\n",
    "        return self._id_to_token.get(index, None)\n",
    "\n",
    "    def save_vocabulary(self, save_directory: str, filename_prefix: str  = None):\n",
    "        vocab_file = os.path.join(save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + \"vocab.txt\")\n",
    "        with open(vocab_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(self.all_tokens))\n",
    "        return (vocab_file,)\n",
    "    \n",
    "    @property\n",
    "    def all_tokens(self) -> List[str]:\n",
    "        return list(self.get_vocab().keys())\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.all_tokens)\n",
    "    \n",
    "class RnaTokenizer(Tokenizer):\n",
    "   \n",
    "    model_input_names = [\"input_ids\"]#, \"attention_mask\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nmers: int = 2,\n",
    "        replace_U_with_T: bool = True,\n",
    "        do_upper_case: bool = True,\n",
    "        model_max_length: int = 30,\n",
    "        model_name: str = \"\",\n",
    "\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            do_upper_case=do_upper_case,\n",
    "            model_max_length=model_max_length,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.replace_U_with_T = replace_U_with_T\n",
    "        self.nmers = nmers\n",
    "        self.model_name = model_name.lower()\n",
    "\n",
    "    def chunkstring_overlap(self, string):\n",
    "        return (\n",
    "            string[0 + i : self.nmers + i] for i in range(0, len(string) - self.nmers + 1, 1)\n",
    "        )\n",
    "    \n",
    "    def _tokenize(self, rnas: str, **kwargs):\n",
    "        if self.do_upper_case:\n",
    "            rnas = rnas.upper()\n",
    "        if self.replace_U_with_T:\n",
    "            rnas = rnas.replace(\"U\", \"T\")\n",
    "\n",
    "        return list(self.chunkstring_overlap(rnas))\n",
    "    \n",
    "    def custom_roll(self,arr, n_shifts_per_row):\n",
    "        '''\n",
    "        shifts each row of a numpy array according to n_shifts_per_row\n",
    "        '''\n",
    "        from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "        m = np.asarray(n_shifts_per_row)\n",
    "        arr_roll = arr[:, [*range(arr.shape[1]),*range(arr.shape[1]-1)]].copy() #need `copy`\n",
    "        strd_0, strd_1 = arr_roll.strides\n",
    "        n = arr.shape[1]\n",
    "        result = as_strided(arr_roll, (*arr.shape, n), (strd_0 ,strd_1, strd_1))\n",
    "\n",
    "        return result[np.arange(arr.shape[0]), (n-m)%n]\n",
    "    \n",
    "    def __call__(\n",
    "        self,\n",
    "        rnas: str,\n",
    "        return_tensors: str = \"pt\",\n",
    "        padding: bool = \"max_length\",\n",
    "        truncation: bool = True,\n",
    "        **kwargs,\n",
    "    ) -> Dict[str, List[int]]:\n",
    "        seq_lens = np.array([len(rna) for rna in rnas])\n",
    "        \n",
    "        result =  super().__call__(\n",
    "            rnas,\n",
    "            return_tensors=return_tensors,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            **kwargs,\n",
    "        )\n",
    "        rna_token_ids = np.array(result[\"input_ids\"])\n",
    "        second_token_ids = np.zeros_like(rna_token_ids)\n",
    "\n",
    "        if 'struct' in self.model_name:\n",
    "            from transforna import fold_sequences\n",
    "            rnas_ss = list(fold_sequences(rnas)['structure_37'].values)\n",
    "            result =  super().__call__(\n",
    "                rnas_ss,\n",
    "                return_tensors=return_tensors,\n",
    "                padding=padding,\n",
    "                truncation=truncation,\n",
    "                **kwargs,\n",
    "            )\n",
    "            second_token_ids = np.array(result[\"input_ids\"])\n",
    "        elif 'rev' in self.model_name:\n",
    "            sample_token_ids_rev = rna_token_ids[:,::-1]\n",
    "            n_zeros = np.count_nonzero(sample_token_ids_rev==0, axis=1)\n",
    "            second_token_ids = self.custom_roll(sample_token_ids_rev, -n_zeros)\n",
    "        \n",
    "        elif 'seq-seq' in self.model_name:\n",
    "            phase0 = rna_token_ids[:,::2]\n",
    "            phase1 = rna_token_ids[:,1::2]\n",
    "            #in case max_length is an odd number phase 0 will be 1 entry larger than phase 1 @ dim=1 \n",
    "            if phase0.shape!= phase1.shape:\n",
    "                phase1 = np.concatenate([phase1,np.zeros(phase1.shape[0])[...,np.newaxis]],axis=1)\n",
    "            rna_token_ids = phase0\n",
    "            second_token_ids = phase1\n",
    "        else:\n",
    "            #seq\n",
    "            pass\n",
    "            \n",
    "\n",
    "        result['input_ids'] = torch.tensor(np.concatenate([rna_token_ids,second_token_ids,seq_lens[...,np.newaxis]],axis=1))\n",
    "\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RnaTokenizer(model_max_length=29,model_name=model_name)\n",
    "tokenizer.add_special_tokens({'pad_token': 'pad'})\n",
    "x = tokenizer(['AACGAAGCTCGACTTTTAAGG'\\\n",
    "            ,'GTCCACCCCAAAGCGTAGG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 5., 15.,  5., 14.,  1.,  4.,  2., 10.,  7.,  8.,  0.,  0.,  0.,  0.,\n",
       "          0., 16.,  4.,  8.,  2., 15., 16., 10., 10.,  5., 13.,  0.,  0.,  0.,\n",
       "          0.,  0., 21.],\n",
       "        [ 9., 11., 16., 11., 12.,  5., 14.,  9.,  8.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  1., 12., 11., 11.,  5.,  8., 15.,  7., 13.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0., 19.]], dtype=torch.float64)}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Yak-hbdx/Seq-Seq-TransfoRNA/commit/14ef8d30ec6ae0dd06304c096e0e800386ad2c21', commit_message='Upload tokenizer', commit_description='', oid='14ef8d30ec6ae0dd06304c096e0e800386ad2c21', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save tokenizer and push to hub\n",
    "tokenizer.push_to_hub(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPASSED\n"
     ]
    }
   ],
   "source": [
    "#load model and tokenizer\n",
    "model = GeneEmbeddModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "tokenizer = RnaTokenizer.from_pretrained(model_name,model_name=model_name)\n",
    "output = tokenizer(['AAAGTCGGAGGTTCGAAGACGATCAGATAC','TTTTCGGAACTGAGGCCATGATTAAGAGGG'])\n",
    "gene_embedd, second_input_embedd, activations,attn_scores_first,attn_scores_second = model(output['input_ids'])\n",
    "#get the idx of the maximum value in the gene_embedd tensor at each row\n",
    "class_ids = torch.argmax(activations,dim=1).numpy()\n",
    "class_labels = model.convert_ids_to_labels(class_ids)\n",
    "#asset ['18S_bin-38', '18S_bin-33']\n",
    "assert class_labels == ['18S_bin-38', '18S_bin-33'], print('\\033[91m' + 'FAILED')\n",
    "print('\\033[92m' + 'PASSED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mFAILED\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m class_labels \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconvert_ids_to_labels(class_ids)\n\u001b[1;32m     10\u001b[0m major_class \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconvert_subclass_to_majorclass(class_labels)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m class_labels \u001b[38;5;241m==\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m28S_bin-80\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmiR-629-3p\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[91m\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFAILED\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[92m\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPASSED\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: None"
     ]
    }
   ],
   "source": [
    "#load model and tokenizer\n",
    "model = GeneEmbeddModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "tokenizer = RnaTokenizer.from_pretrained(model_name,model_name=model_name)\n",
    "output = tokenizer(['AACGAAGCTCGACTTTTAAGG','GTCCACCCCAAAGCGTAGG'])\n",
    "gene_embedd, second_input_embedd, activations,attn_scores_first,attn_scores_second = model(output['input_ids'])\n",
    "#get the idx of the maximum value in the gene_embedd tensor at each row\n",
    "class_ids = torch.argmax(activations,dim=1).numpy()\n",
    "class_labels = model.convert_ids_to_labels(class_ids)\n",
    "major_class = model.convert_subclass_to_majorclass(class_labels)\n",
    "assert class_labels == ['28S_bin-80', 'miR-629-3p'], print('\\033[91m' + 'FAILED')\n",
    "print('\\033[92m' + 'PASSED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transforna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
